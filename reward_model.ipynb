{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e61a6af-00f8-4ad0-ac8e-c07a394ce3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from resnet_features import resnet18_features, resnet34_features, resnet50_features, resnet101_features, resnet152_features\n",
    "from densenet_features import densenet121_features, densenet161_features, densenet169_features, densenet201_features\n",
    "from vgg_features import vgg11_features, vgg11_bn_features, vgg13_features, vgg13_bn_features, vgg16_features, vgg16_bn_features,\\\n",
    "                         vgg19_features, vgg19_bn_features\n",
    "\n",
    "from receptive_field import compute_proto_layer_rf_info_v2\n",
    "\n",
    "from settings import img_size\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle as pkl\n",
    "import skimage as sk\n",
    "import skimage.io as skio\n",
    "from preference_model import construct_PrefNet, paired_cross_entropy_loss, PrefNet\n",
    "\n",
    "# book keeping namings and code\n",
    "from settings import base_architecture, img_size, prototype_shape, num_classes, \\\n",
    "                     prototype_activation_function, add_on_layers_type, experiment_run\n",
    "\n",
    "from preprocess import mean, std, preprocess_input_function\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e15376-1146-4353-b0d2-4d35fa3e00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_architecture_to_features = {'resnet18': resnet18_features,\n",
    "                                 'resnet34': resnet34_features,\n",
    "                                 'resnet50': resnet50_features,\n",
    "                                 'resnet101': resnet101_features,\n",
    "                                 'resnet152': resnet152_features,\n",
    "                                 'densenet121': densenet121_features,\n",
    "                                 'densenet161': densenet161_features,\n",
    "                                 'densenet169': densenet169_features,\n",
    "                                 'densenet201': densenet201_features,\n",
    "                                 'vgg11': vgg11_features,\n",
    "                                 'vgg11_bn': vgg11_bn_features,\n",
    "                                 'vgg13': vgg13_features,\n",
    "                                 'vgg13_bn': vgg13_bn_features,\n",
    "                                 'vgg16': vgg16_features,\n",
    "                                 'vgg16_bn': vgg16_bn_features,\n",
    "                                 'vgg19': vgg19_features,\n",
    "                                 'vgg19_bn': vgg19_bn_features}\n",
    "\n",
    "\n",
    "class PrefNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_features, pattern_features, img_size, prototype_shape,\n",
    "                 proto_layer_rf_info, num_classes, init_weights=True,\n",
    "                 prototype_activation_function='log',\n",
    "                 add_on_layers_type='bottleneck', \n",
    "                k = 3):\n",
    "\n",
    "        super(PrefNet, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.prototype_shape = prototype_shape\n",
    "        self.num_prototypes = prototype_shape[0]\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = 1e-4\n",
    "        self.k = k\n",
    "        \n",
    "        # this has to be named features to allow the precise loading\n",
    "        self.img_features = img_features\n",
    "        self.pattern_features = pattern_features\n",
    "        \n",
    "        \n",
    "        \n",
    "        features_name = str(self.img_features).upper()\n",
    "        if features_name.startswith('VGG') or features_name.startswith('RES'):\n",
    "            first_add_on_layer_in_channels = \\\n",
    "                [i for i in img_features.modules() if isinstance(i, nn.Conv2d)][-1].out_channels\n",
    "        elif features_name.startswith('DENSE'):\n",
    "            first_add_on_layer_in_channels = \\\n",
    "                [i for i in img_features.modules() if isinstance(i, nn.BatchNorm2d)][-1].num_features\n",
    "        else:\n",
    "            raise Exception('other base base_architecture NOT implemented')\n",
    "\n",
    "        if add_on_layers_type == 'bottleneck':\n",
    "            add_on_layers = []\n",
    "            current_in_channels = first_add_on_layer_in_channels\n",
    "            while (current_in_channels > self.prototype_shape[1]) or (len(add_on_layers) == 0):\n",
    "                current_out_channels = max(self.prototype_shape[1], (current_in_channels // 2))\n",
    "                add_on_layers.append(nn.Conv2d(in_channels=current_in_channels,\n",
    "                                               out_channels=current_out_channels,\n",
    "                                               kernel_size=1))\n",
    "                add_on_layers.append(nn.ReLU())\n",
    "                add_on_layers.append(nn.Conv2d(in_channels=current_out_channels,\n",
    "                                               out_channels=current_out_channels,\n",
    "                                               kernel_size=1))\n",
    "                if current_out_channels > self.prototype_shape[1]:\n",
    "                    add_on_layers.append(nn.ReLU())\n",
    "                else:\n",
    "                    assert(current_out_channels == self.prototype_shape[1])\n",
    "                    add_on_layers.append(nn.Sigmoid())\n",
    "                current_in_channels = current_in_channels // 2\n",
    "            self.img_add_on_layers = nn.Sequential(*add_on_layers)\n",
    "        else:\n",
    "            self.img_add_on_layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=self.prototype_shape[1], out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        features_name = str(self.pattern_features).upper()\n",
    "        if features_name.startswith('VGG') or features_name.startswith('RES'):\n",
    "            first_add_on_layer_in_channels = \\\n",
    "                [i for i in pattern_features.modules() if isinstance(i, nn.Conv2d)][-1].out_channels\n",
    "        elif features_name.startswith('DENSE'):\n",
    "            first_add_on_layer_in_channels = \\\n",
    "                [i for i in pattern_features.modules() if isinstance(i, nn.BatchNorm2d)][-1].num_features\n",
    "        else:\n",
    "            raise Exception('other base base_architecture NOT implemented')\n",
    "\n",
    "        if add_on_layers_type == 'bottleneck':\n",
    "            add_on_layers = []\n",
    "            current_in_channels = first_add_on_layer_in_channels\n",
    "            while (current_in_channels > self.prototype_shape[1]) or (len(add_on_layers) == 0):\n",
    "                current_out_channels = max(self.prototype_shape[1], (current_in_channels // 2))\n",
    "                add_on_layers.append(nn.Conv2d(in_channels=current_in_channels,\n",
    "                                               out_channels=current_out_channels,\n",
    "                                               kernel_size=1))\n",
    "                add_on_layers.append(nn.ReLU())\n",
    "                add_on_layers.append(nn.Conv2d(in_channels=current_out_channels,\n",
    "                                               out_channels=current_out_channels,\n",
    "                                               kernel_size=1))\n",
    "                if current_out_channels > self.prototype_shape[1]:\n",
    "                    add_on_layers.append(nn.ReLU())\n",
    "                else:\n",
    "                    assert(current_out_channels == self.prototype_shape[1])\n",
    "                    add_on_layers.append(nn.Sigmoid())\n",
    "                current_in_channels = current_in_channels // 2\n",
    "            self.pattern_add_on_layers = nn.Sequential(*add_on_layers)\n",
    "        else:\n",
    "            self.pattern_add_on_layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=self.prototype_shape[1], out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "            \n",
    "            \n",
    "            \n",
    "#         self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape),\n",
    "#                                               requires_grad=True)\n",
    "\n",
    "#         # do not make this just a tensor,\n",
    "#         # since it will not be moved automatically to gpu\n",
    "#         self.ones = nn.Parameter(torch.ones(self.prototype_shape),\n",
    "#                                  requires_grad=False)\n",
    "\n",
    "#         self.last_layer = nn.Linear(self.num_prototypes, self.num_classes,\n",
    "#                                     bias=False) # do not use bias\n",
    "\n",
    "\n",
    "        self.img_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.pattern_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.img_fc = nn.Linear(512 * 7 * 7, 3200)\n",
    "        self.pattern_fc = nn.Linear(512 * 7 * 7, 3200)\n",
    "        self.fc1 = nn.Linear(6400, 512)\n",
    "        self.fc2 = nn.Linear(512, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "            \n",
    "            \n",
    "    def conv_features(self, x):\n",
    "        '''\n",
    "        the feature input to prototype layer\n",
    "        '''\n",
    "        # Insert k and then img size\n",
    "        x = self.features(x)\n",
    "        x = self.add_on_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, p):\n",
    "        # (N, 512, 7, 7)\n",
    "        #x = self.conv_features(x)\n",
    "        x = self.img_features(x)\n",
    "        x = self.img_add_on_layers(x)\n",
    "        #x = self.img_conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(x.shape)\n",
    "        x = self.img_fc(x)\n",
    "        #print(\"img_conv out shape: \", x.shape)\n",
    "        \n",
    "        #p = self.conv_features(p)\n",
    "        p = self.pattern_features(p)\n",
    "        p = self.pattern_add_on_layers(p)\n",
    "        p = torch.flatten(p, 1)\n",
    "        #p = self.pattern_conv(p)\n",
    "        p = self.pattern_fc(p)\n",
    "        #print(\"pattern_conv out shape: \", p.shape)\n",
    "        \n",
    "        out = torch.cat((x, p), dim=1)\n",
    "        #print(\"cat out shape: \", out.shape)\n",
    "        out = torch.flatten(out, 1) # flatten all dimensions except batch\n",
    "        #print(\"flatten out shape: \", out.shape)\n",
    "        \n",
    "       \n",
    "        out = torch.sigmoid(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.img_add_on_layers.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # every init technique has an underscore _ in the name\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        for m in self.pattern_add_on_layers.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # every init technique has an underscore _ in the name\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def construct_PrefNet(base_architecture, pretrained=True, img_size=224,\n",
    "                    prototype_shape=(2000, 512, 1, 1), num_classes=200,\n",
    "                    prototype_activation_function='log',\n",
    "                    add_on_layers_type='bottleneck',\n",
    "                    k = 3):\n",
    "    img_features = base_architecture_to_features[base_architecture](pretrained=pretrained)\n",
    "    pattern_features = base_architecture_to_features[base_architecture](pretrained=pretrained)\n",
    "    layer_filter_sizes, layer_strides, layer_paddings = img_features.conv_info()\n",
    "    proto_layer_rf_info = compute_proto_layer_rf_info_v2(img_size=img_size,\n",
    "                                                         layer_filter_sizes=layer_filter_sizes,\n",
    "                                                         layer_strides=layer_strides,\n",
    "                                                         layer_paddings=layer_paddings,\n",
    "                                                         prototype_kernel_size=prototype_shape[2])\n",
    "    return PrefNet(img_features=img_features,\n",
    "                 pattern_features=pattern_features,\n",
    "                 img_size=img_size,\n",
    "                 prototype_shape=prototype_shape,\n",
    "                 proto_layer_rf_info=proto_layer_rf_info,\n",
    "                 num_classes=num_classes,\n",
    "                 init_weights=True,\n",
    "                 prototype_activation_function=prototype_activation_function,\n",
    "                 add_on_layers_type=add_on_layers_type,\n",
    "                 k = k)\n",
    "\n",
    "\n",
    "def paired_cross_entropy_loss(out1, out2, targets):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(targets)):\n",
    "        \n",
    "        if targets[i] == -1:\n",
    "            p1 = torch.exp(out1[i])/(torch.exp(out1[i]) + torch.exp(out2[i]))\n",
    "            loss = - torch.log(p1)\n",
    "        elif targets[i] == 1:\n",
    "            p2 = torch.exp(out2[i])/(torch.exp(out1[i]) + torch.exp(out2[i]))\n",
    "            loss = - torch.log(p2)\n",
    "\n",
    "        else:\n",
    "            p1 = torch.exp(out1[i])/(torch.exp(out1[i]) + torch.exp(out2[i]))\n",
    "            p2 = torch.exp(out2[i])/(torch.exp(out1[i]) + torch.exp(out2[i]))\n",
    "            loss = - (0.5*torch.log(p1) + 0.5*torch.log(p2))\n",
    "            \n",
    "        total_loss += loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39926c21-9702-4a73-82f3-ddfbfcdf7098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec93869e-6770-4067-926d-99e8b348d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=mean,\n",
    "                                 std=std)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize(size=(img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11bcc1c8-6320-47bf-8b97-8301c097d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "csv_name = \"./human_comparisons/rating_s=5_k=1_700_random_1.csv\"\n",
    "if os.path.exists(csv_name):\n",
    "    comp_df = pd.read_csv(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b96dbfc4-5a99-4b41-ae1e-d2dcb5016e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 76.,   0.,  51.,   0.,   0.,  44.,   0., 120.,   0., 409.]),\n",
       " array([1. , 1.4, 1.8, 2.2, 2.6, 3. , 3.4, 3.8, 4.2, 4.6, 5. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(comp_df['rating'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c9cea5-01bc-48ba-bb66-8a6042fc0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = \"./human_comparisons/rating_s=5_k=1_500.csv\"\n",
    "if os.path.exists(csv_name):\n",
    "    comp_df = pd.read_csv(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8590cb1f-ef35-4624-9b7d-14bd6b13128b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 25.,   0.,  26.,   0.,   0.,  62.,   0., 100.,   0., 287.]),\n",
       " array([1. , 1.4, 1.8, 2.2, 2.6, 3. , 3.4, 3.8, 4.2, 4.6, 5. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(comp_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6d69b5-9fd4-41a0-b87b-67e89bcffa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96462\n",
      "5668\n"
     ]
    }
   ],
   "source": [
    "split = 0.8\n",
    "df_len = len(comp_df)\n",
    "train_set = []\n",
    "test_set = []\n",
    "split_idx = int(df_len*split)\n",
    "for i in range(split_idx):\n",
    "    for j in range(i+1, split_idx):\n",
    "        if comp_df.iloc[i]['rating'] > comp_df.iloc[j]['rating']:\n",
    "            train_set.append([i, j, -1])\n",
    "        elif comp_df.iloc[i]['rating'] < comp_df.iloc[j]['rating']:\n",
    "            train_set.append([i, j, 1])\n",
    "            \n",
    "for i in range(split_idx, df_len):\n",
    "    for j in range(i+1, df_len):\n",
    "        if comp_df.iloc[i]['rating'] > comp_df.iloc[j]['rating']:\n",
    "            test_set.append([i, j, -1])\n",
    "        elif comp_df.iloc[i]['rating'] < comp_df.iloc[j]['rating']:\n",
    "            test_set.append([i, j, 1])\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4352e7f6-affc-443c-83f1-966f474fb79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nanother way of splitting data\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "another way of splitting data\n",
    "'''\n",
    "#split = 0.7\n",
    "#train_set = training_set[:int(len(training_set) * split)]\n",
    "#test_set = training_set[int(len(training_set) * split):]\n",
    "#print(len(train_set))\n",
    "#print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c119e25-3c17-41fb-b90c-d830702acc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "patterns = []\n",
    "for i in range(df_len):\n",
    "    img = './human_comparisons/feedback_images/k=1_random/original_imgs/' + comp_df.iloc[i]['imgid'] + '.png'\n",
    "    img = plt.imread(img)[:, :, :3]\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    images.append(torch.from_numpy(np.array([img])))\n",
    "    pattern = './human_comparisons/feedback_images/k=1_random/patterns/' + comp_df.iloc[i]['imgid'] + '.npy'\n",
    "    pattern = np.load(pattern)\n",
    "    pattern = np.array([pattern, pattern, pattern])\n",
    "    patterns.append(torch.from_numpy(np.array([pattern])))\n",
    "print(len(images))\n",
    "print(images[100].shape)\n",
    "print(patterns[100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0696fc8-0923-49be-bf44-3e0660b85681",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefnet = construct_PrefNet(\"resnet50\")\n",
    "prefnet.to(device)\n",
    "prefnet.train()\n",
    "pref_optimizer = optim.Adam([{'params': prefnet.img_features.parameters(), 'lr': 1e-4}, {'params': prefnet.pattern_features.parameters(), 'lr': 1e-4}, \n",
    "                             {'params': prefnet.img_fc.parameters(), 'lr': 1e-4}, {'params': prefnet.pattern_fc.parameters(), 'lr': 1e-4},\n",
    "                             {'params': prefnet.img_add_on_layers.parameters(), 'lr': 1e-4}, {'params': prefnet.pattern_add_on_layers.parameters(), 'lr': 1e-4},\n",
    "                             {'params': prefnet.img_conv.parameters(), 'lr': 1e-4}, {'params': prefnet.pattern_conv.parameters(), 'lr': 1e-4}, \n",
    "                             {'params': prefnet.fc1.parameters(), 'lr': 1e-4}, {'params': prefnet.fc2.parameters(), 'lr': 1e-4}, {'params': prefnet.fc3.parameters(), 'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a540a57-6b9f-418c-bf90-6a4da17d3c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0322]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefnet(images[10].cuda(), patterns[10].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd22e4b8-0bb0-4563-a51b-a5cbacd06d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45935b65-b1ce-4334-b804-877048fcb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_optimizer = optim.Adam([{'params': prefnet.img_conv.parameters(), 'lr': 1e-5}, {'params': prefnet.pattern_conv.parameters(), 'lr': 1e-5}, {'params': prefnet.fc1.parameters(), 'lr': 1e-5},\n",
    "                            {'params': prefnet.fc2.parameters(), 'lr': 1e-5}, {'params': prefnet.fc3.parameters(), 'lr': 1e-5}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d8b37a-97ad-4817-8aab-f903dc2fb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward_model(prefnet, test_set, images, patterns):\n",
    "    acc = []\n",
    "    #error_images = []\n",
    "    error_count = 0\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        left_idx, right_idx, target = test_set[i]\n",
    "        left_img, right_img = images[left_idx], images[right_idx]\n",
    "        left_pattern, right_pattern = patterns[left_idx], patterns[right_idx]\n",
    "        target = torch.tensor(target).cuda().float()\n",
    "\n",
    "        out1 = prefnet(left_img.cuda().float(), left_pattern.cuda().float())\n",
    "        out2 = prefnet(right_img.cuda().float(), right_pattern.cuda().float())\n",
    "        #print(out1)\n",
    "        #print(out2)\n",
    "\n",
    "\n",
    "        if out1 > out2:\n",
    "            y_pred = -1\n",
    "\n",
    "        else:\n",
    "            y_pred = 1\n",
    "\n",
    "        #print(y_pred)\n",
    "        #print(\"\")\n",
    "        if y_pred == target:\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            #error_images.append((i, y_pred, target))\n",
    "            error_count += 1\n",
    "            acc.append(0)\n",
    "            \n",
    "    return np.mean(acc), error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ebf1dc7-bd65-40ef-9107-ebc954e5aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 22.184793\n",
      "0 99 450\n",
      "0 100 14.297962\n",
      "0 199 252\n",
      "0 200 5.1353197\n",
      "0 299 209\n",
      "0 300 5.857233\n",
      "0 399 175\n",
      "0 400 5.807886\n",
      "0 499 163\n",
      "0 500 3.5191991\n",
      "0 599 170\n",
      "0 600 2.889525\n",
      "0 699 165\n",
      "0 700 2.675007\n",
      "0 799 110\n",
      "0 800 3.3982656\n",
      "0 899 80\n",
      "0 900 5.0861344\n",
      "0 999 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:45<00:00, 34.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 999 0.6367325335215244 2059\n",
      "0 1000 0.9982669\n",
      "0 1099 83\n",
      "0 1100 2.6490378\n",
      "0 1199 82\n",
      "0 1200 2.9638603\n",
      "0 1299 67\n",
      "0 1300 0.57401335\n",
      "0 1399 67\n",
      "0 1400 1.6640669\n",
      "0 1499 97\n",
      "0 1500 5.312115\n",
      "0 1599 118\n",
      "0 1600 1.9212143\n",
      "0 1699 133\n",
      "0 1700 10.714208\n",
      "0 1799 91\n",
      "0 1800 2.4598808\n",
      "0 1899 40\n",
      "0 1900 1.9612269\n",
      "0 1999 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:45<00:00, 34.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1999 0.6337332392378264 2076\n",
      "0 2000 1.4717791\n",
      "0 2099 20\n",
      "0 2100 1.1694398\n",
      "0 2199 14\n",
      "0 2200 3.4615276\n",
      "0 2299 25\n",
      "0 2300 1.9244483\n",
      "0 2399 20\n",
      "0 2400 0.9921782\n",
      "0 2499 20\n",
      "0 2500 0.6528253\n",
      "0 2599 12\n",
      "0 2600 1.633644\n",
      "0 2699 23\n",
      "0 2700 0.70512575\n",
      "0 2799 19\n",
      "0 2800 0.33356002\n",
      "0 2899 21\n",
      "0 2900 1.114428\n",
      "0 2999 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2999 0.6886026817219478 1765\n",
      "0 3000 0.48828435\n",
      "1 0 0.59915525\n",
      "1 99 19\n",
      "1 100 1.1854101\n",
      "1 199 125\n",
      "1 200 3.2929416\n",
      "1 299 72\n",
      "1 300 0.87660706\n",
      "1 399 54\n",
      "1 400 1.8347942\n",
      "1 499 23\n",
      "1 500 1.6536994\n",
      "1 599 22\n",
      "1 600 1.3824136\n",
      "1 699 12\n",
      "1 700 0.37911347\n",
      "1 799 19\n",
      "1 800 0.59483147\n",
      "1 899 18\n",
      "1 900 1.0613755\n",
      "1 999 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 999 0.6441425546930134 2017\n",
      "1 1000 0.9689445\n",
      "1 1099 13\n",
      "1 1100 1.9752756\n",
      "1 1199 53\n",
      "1 1200 2.3789754\n",
      "1 1299 30\n",
      "1 1300 0.39389306\n",
      "1 1399 22\n",
      "1 1400 0.70361847\n",
      "1 1499 16\n",
      "1 1500 1.8075132\n",
      "1 1599 11\n",
      "1 1600 0.32634842\n",
      "1 1699 5\n",
      "1 1700 0.3237077\n",
      "1 1799 8\n",
      "1 1800 0.33987722\n",
      "1 1899 12\n",
      "1 1900 0.34683233\n",
      "1 1999 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1999 0.6137967537050106 2189\n",
      "1 2000 0.20938529\n",
      "1 2099 7\n",
      "1 2100 0.18075247\n",
      "1 2199 7\n",
      "1 2200 1.100905\n",
      "1 2299 18\n",
      "1 2300 0.45912367\n",
      "1 2399 10\n",
      "1 2400 0.22526167\n",
      "1 2499 14\n",
      "1 2500 0.7180519\n",
      "1 2599 5\n",
      "1 2600 0.25828138\n",
      "1 2699 4\n",
      "1 2700 0.15080997\n",
      "1 2799 14\n",
      "1 2800 0.16626477\n",
      "1 2899 11\n",
      "1 2900 2.1143417\n",
      "1 2999 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2999 0.613091037402964 2193\n",
      "1 3000 3.9885955\n",
      "2 0 2.3245196\n",
      "2 99 126\n",
      "2 100 1.5453582\n",
      "2 199 67\n",
      "2 200 1.0276006\n",
      "2 299 60\n",
      "2 300 1.7047565\n",
      "2 399 24\n",
      "2 400 0.640802\n",
      "2 499 18\n",
      "2 500 1.0494665\n",
      "2 599 16\n",
      "2 600 0.48829037\n",
      "2 699 11\n",
      "2 700 0.92843574\n",
      "2 799 15\n",
      "2 800 0.37774855\n",
      "2 899 14\n",
      "2 900 0.14227578\n",
      "2 999 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 999 0.5950952717007763 2295\n",
      "2 1000 0.16373672\n",
      "2 1099 14\n",
      "2 1100 0.29884928\n",
      "2 1199 16\n",
      "2 1200 0.9934118\n",
      "2 1299 13\n",
      "2 1300 0.08022984\n",
      "2 1399 11\n",
      "2 1400 0.9101034\n",
      "2 1499 14\n",
      "2 1500 0.094481476\n",
      "2 1599 13\n",
      "2 1600 3.3656678\n",
      "2 1699 31\n",
      "2 1700 1.0917705\n",
      "2 1799 12\n",
      "2 1800 2.3603432\n",
      "2 1899 14\n",
      "2 1900 0.24940568\n",
      "2 1999 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1999 0.6157374735356387 2178\n",
      "2 2000 0.25845435\n",
      "2 2099 9\n",
      "2 2100 0.47591484\n",
      "2 2199 9\n",
      "2 2200 0.17844267\n",
      "2 2299 9\n",
      "2 2300 0.17417417\n",
      "2 2399 9\n",
      "2 2400 0.070376664\n",
      "2 2499 5\n",
      "2 2500 0.38370702\n",
      "2 2599 11\n",
      "2 2600 2.7886994\n",
      "2 2699 13\n",
      "2 2700 0.42985508\n",
      "2 2799 10\n",
      "2 2800 0.08183704\n",
      "2 2899 14\n",
      "2 2900 0.11473453\n",
      "2 2999 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2999 0.6524347212420607 1970\n",
      "2 3000 0.03782856\n",
      "3 0 0.35016716\n",
      "3 99 18\n",
      "3 100 0.31950384\n",
      "3 199 41\n",
      "3 200 2.1980448\n",
      "3 299 31\n",
      "3 300 2.1630924\n",
      "3 399 31\n",
      "3 400 0.9037249\n",
      "3 499 82\n",
      "3 500 0.985262\n",
      "3 599 42\n",
      "3 600 0.9837992\n",
      "3 699 32\n",
      "3 700 0.14244045\n",
      "3 799 18\n",
      "3 800 0.3171933\n",
      "3 899 15\n",
      "3 900 0.43045956\n",
      "3 999 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 999 0.5984474241354976 2276\n",
      "3 1000 0.25907588\n",
      "3 1099 7\n",
      "3 1100 0.8069564\n",
      "3 1199 8\n",
      "3 1200 0.06784931\n",
      "3 1299 9\n",
      "3 1300 0.015485441\n",
      "3 1399 8\n",
      "3 1400 0.045774408\n",
      "3 1499 13\n",
      "3 1500 0.28750274\n",
      "3 1599 8\n",
      "3 1600 0.11569345\n",
      "3 1699 3\n",
      "3 1700 0.075682566\n",
      "3 1799 9\n",
      "3 1800 0.08297684\n",
      "3 1899 11\n",
      "3 1900 0.4398191\n",
      "3 1999 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1999 0.6044460127028934 2242\n",
      "3 2000 0.112415776\n",
      "3 2099 14\n",
      "3 2100 0.7114613\n",
      "3 2199 7\n",
      "3 2200 0.072063126\n",
      "3 2299 19\n",
      "3 2300 1.7817461\n",
      "3 2399 16\n",
      "3 2400 0.27942163\n",
      "3 2499 14\n",
      "3 2500 0.0912195\n",
      "3 2599 12\n",
      "3 2600 0.51357615\n",
      "3 2699 11\n",
      "3 2700 4.8752246\n",
      "3 2799 16\n",
      "3 2800 0.033846274\n",
      "3 2899 9\n",
      "3 2900 1.0615565\n",
      "3 2999 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2999 0.6547282992237121 1957\n",
      "3 3000 0.21489385\n",
      "4 0 0.029362269\n",
      "4 99 6\n",
      "4 100 0.06250813\n",
      "4 199 10\n",
      "4 200 0.048340037\n",
      "4 299 11\n",
      "4 300 0.25580907\n",
      "4 399 21\n",
      "4 400 0.503122\n",
      "4 499 70\n",
      "4 500 1.0480794\n",
      "4 599 36\n",
      "4 600 0.3887238\n",
      "4 699 67\n",
      "4 700 1.3717974\n",
      "4 799 27\n",
      "4 800 2.0941615\n",
      "4 899 23\n",
      "4 900 0.5938817\n",
      "4 999 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:43<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 999 0.6674311926605505 1885\n",
      "4 1000 2.6442757\n",
      "4 1099 14\n",
      "4 1100 0.14226429\n",
      "4 1199 6\n",
      "4 1200 0.52649385\n",
      "4 1299 9\n",
      "4 1300 0.24149111\n",
      "4 1399 10\n",
      "4 1400 0.1269612\n",
      "4 1499 5\n",
      "4 1500 0.11213868\n",
      "4 1599 11\n",
      "4 1600 0.051074903\n",
      "4 1699 12\n",
      "4 1700 0.1102878\n",
      "4 1799 16\n",
      "4 1800 0.12266795\n",
      "4 1899 9\n",
      "4 1900 0.86800647\n",
      "4 1999 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:44<00:00, 34.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1999 0.6339096683133381 2075\n",
      "4 2000 0.15081117\n",
      "4 2099 18\n",
      "4 2100 0.124191396\n",
      "4 2199 22\n",
      "4 2200 6.154881\n",
      "4 2299 62\n",
      "4 2300 2.2167327\n",
      "4 2399 25\n",
      "4 2400 0.94078183\n",
      "4 2499 13\n",
      "4 2500 0.10738749\n",
      "4 2599 25\n",
      "4 2600 0.39907816\n",
      "4 2699 19\n",
      "4 2700 0.15636638\n",
      "4 2799 10\n",
      "4 2800 0.05025979\n",
      "4 2899 17\n",
      "4 2900 0.6114581\n",
      "4 2999 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5668/5668 [02:44<00:00, 34.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2999 0.629498941425547 2100\n",
      "4 3000 0.10164178\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    shuffled_idx = np.random.permutation(len(train_set))\n",
    "    for batch_i in range(len(train_set)//batch_size + 1):\n",
    "        if batch_i % 100 == 0:\n",
    "            last_100_losses = []\n",
    "            last_100_error_count = 0\n",
    "        idx = shuffled_idx[batch_i*batch_size:(batch_i+1)*batch_size]\n",
    "        \n",
    "        left_imgs = torch.zeros((batch_size, 3, 224, 224))\n",
    "        right_imgs = torch.zeros((batch_size, 3, 224, 224))\n",
    "        left_patterns = torch.zeros((batch_size, 3, 224, 224))\n",
    "        right_patterns = torch.zeros((batch_size, 3, 224, 224))\n",
    "        targets = []\n",
    "        for i in range(len(idx)):\n",
    "            index = idx[i]\n",
    "            left_imgs[i] = images[train_set[index][0]][0]\n",
    "            right_imgs[i] = images[train_set[index][1]][0]\n",
    "            targets.append(train_set[index][2])\n",
    "            left_patterns[i] = patterns[train_set[index][0]][0]\n",
    "            right_patterns[i] = patterns[train_set[index][1]][0]\n",
    "        \n",
    "        targets = torch.tensor(targets).cuda().float()\n",
    "        \n",
    "        out1 = prefnet(left_imgs.cuda().float(), left_patterns.cuda().float())\n",
    "        out2 = prefnet(right_imgs.cuda().float(), right_patterns.cuda().float())\n",
    "\n",
    "        \n",
    "        pref_optimizer.zero_grad()   \n",
    "        \n",
    "        for i in range(len(targets)):\n",
    "            if out1[i] > out2[i] and targets[i] == 1:\n",
    "                last_100_error_count += 1\n",
    "\n",
    "            elif out1[i] < out2[i] and targets[i] == -1:\n",
    "                last_100_error_count += 1\n",
    "                \n",
    "        loss = paired_cross_entropy_loss(out1, out2, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        pref_optimizer.step()   \n",
    "        \n",
    "        last_100_losses.append(loss.data.cpu().numpy()[0])\n",
    "        \n",
    "        if batch_i % 100 == 0:\n",
    "            print(epoch, batch_i, np.sum(last_100_losses))\n",
    "        if batch_i % 100 == 99:\n",
    "            print(epoch, batch_i, last_100_error_count)\n",
    "        if batch_i % 1000 == 999:\n",
    "            test_acc, test_error_count = test_reward_model(prefnet, test_set, images, patterns)\n",
    "            print(epoch, batch_i, test_acc, test_error_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b38db82b-b474-4a11-b8ef-666e667d0b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "59b51726-b9ee-4adb-92bc-4d799f48cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefNet(nn.Module):\n",
    "\n",
    "    def __init__(self, features, img_size, prototype_shape,\n",
    "                 proto_layer_rf_info, num_classes, init_weights=True,\n",
    "                 prototype_activation_function='log',\n",
    "                 add_on_layers_type='bottleneck', \n",
    "                k = 3):\n",
    "\n",
    "        super(PrefNet, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.prototype_shape = prototype_shape\n",
    "        self.num_prototypes = prototype_shape[0]\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = 1e-4\n",
    "        self.k = k\n",
    "        \n",
    "        # this has to be named features to allow the precise loading\n",
    "        self.features = features\n",
    "\n",
    "        features_name = str(self.features).upper()\n",
    "        if features_name.startswith('VGG') or features_name.startswith('RES'):\n",
    "            first_add_on_layer_in_channels = \\\n",
    "                [i for i in features.modules() if isinstance(i, nn.Conv2d)][-1].out_channels\n",
    "        elif features_name.startswith('DENSE'):\n",
    "            first_add_on_layer_in_channels = \\\n",
    "                [i for i in features.modules() if isinstance(i, nn.BatchNorm2d)][-1].num_features\n",
    "        else:\n",
    "            raise Exception('other base base_architecture NOT implemented')\n",
    "\n",
    "        if add_on_layers_type == 'bottleneck':\n",
    "            add_on_layers = []\n",
    "            current_in_channels = first_add_on_layer_in_channels\n",
    "            while (current_in_channels > self.prototype_shape[1]) or (len(add_on_layers) == 0):\n",
    "                current_out_channels = max(self.prototype_shape[1], (current_in_channels // 2))\n",
    "                add_on_layers.append(nn.Conv2d(in_channels=current_in_channels,\n",
    "                                               out_channels=current_out_channels,\n",
    "                                               kernel_size=1))\n",
    "                add_on_layers.append(nn.ReLU())\n",
    "                add_on_layers.append(nn.Conv2d(in_channels=current_out_channels,\n",
    "                                               out_channels=current_out_channels,\n",
    "                                               kernel_size=1))\n",
    "                if current_out_channels > self.prototype_shape[1]:\n",
    "                    add_on_layers.append(nn.ReLU())\n",
    "                else:\n",
    "                    assert(current_out_channels == self.prototype_shape[1])\n",
    "                    add_on_layers.append(nn.Sigmoid())\n",
    "                current_in_channels = current_in_channels // 2\n",
    "            self.add_on_layers = nn.Sequential(*add_on_layers)\n",
    "        else:\n",
    "            self.add_on_layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=self.prototype_shape[1], out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "        \n",
    "#         self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape),\n",
    "#                                               requires_grad=True)\n",
    "\n",
    "#         # do not make this just a tensor,\n",
    "#         # since it will not be moved automatically to gpu\n",
    "#         self.ones = nn.Parameter(torch.ones(self.prototype_shape),\n",
    "#                                  requires_grad=False)\n",
    "\n",
    "#         self.last_layer = nn.Linear(self.num_prototypes, self.num_classes,\n",
    "#                                     bias=False) # do not use bias\n",
    "\n",
    "\n",
    "        self.img_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.pattern_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.fc1 = nn.Linear(6400, 512)\n",
    "        self.fc2 = nn.Linear(512, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "            \n",
    "            \n",
    "    def conv_features(self, x):\n",
    "        '''\n",
    "        the feature input to prototype layer\n",
    "        '''\n",
    "        # Insert k and then img size\n",
    "        x = self.features(x)\n",
    "        x = self.add_on_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, p):\n",
    "        # (N, 512, 7, 7)\n",
    "        x = self.conv_features(x)\n",
    "        x = self.img_conv(x)\n",
    "        #print(\"img_conv out shape: \", x.shape)\n",
    "        \n",
    "        p = self.conv_features(p)\n",
    "        p = self.pattern_conv(p)\n",
    "        #print(\"pattern_conv out shape: \", p.shape)\n",
    "        \n",
    "        out = torch.cat((x, p), dim=1)\n",
    "        #print(\"cat out shape: \", out.shape)\n",
    "        out = torch.flatten(out, 1) # flatten all dimensions except batch\n",
    "        #print(\"flatten out shape: \", out.shape)\n",
    "        \n",
    "       \n",
    "        out = torch.sigmoid(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.add_on_layers.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # every init technique has an underscore _ in the name\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def construct_PrefNet(base_architecture, pretrained=True, img_size=224,\n",
    "                    prototype_shape=(2000, 512, 1, 1), num_classes=200,\n",
    "                    prototype_activation_function='log',\n",
    "                    add_on_layers_type='bottleneck',\n",
    "                    k = 3):\n",
    "    features = base_architecture_to_features[base_architecture](pretrained=pretrained)\n",
    "    layer_filter_sizes, layer_strides, layer_paddings = features.conv_info()\n",
    "    proto_layer_rf_info = compute_proto_layer_rf_info_v2(img_size=img_size,\n",
    "                                                         layer_filter_sizes=layer_filter_sizes,\n",
    "                                                         layer_strides=layer_strides,\n",
    "                                                         layer_paddings=layer_paddings,\n",
    "                                                         prototype_kernel_size=prototype_shape[2])\n",
    "    return PrefNet(features=features,\n",
    "                 img_size=img_size,\n",
    "                 prototype_shape=prototype_shape,\n",
    "                 proto_layer_rf_info=proto_layer_rf_info,\n",
    "                 num_classes=num_classes,\n",
    "                 init_weights=True,\n",
    "                 prototype_activation_function=prototype_activation_function,\n",
    "                 add_on_layers_type=add_on_layers_type,\n",
    "                 k = k)\n",
    "\n",
    "\n",
    "def paired_cross_entropy_loss(out1, out2, target):\n",
    "    if target == -1:\n",
    "        p1 = torch.exp(out1)/(torch.exp(out1) + torch.exp(out2))\n",
    "        loss = - torch.log(p1)\n",
    "    elif target == 1:\n",
    "        p2 = torch.exp(out2)/(torch.exp(out1) + torch.exp(out2))\n",
    "        loss = - torch.log(p2)\n",
    "        \n",
    "    else:\n",
    "        p1 = torch.exp(out1)/(torch.exp(out1) + torch.exp(out2))\n",
    "        p2 = torch.exp(out2)/(torch.exp(out1) + torch.exp(out2))\n",
    "        \n",
    "        loss = - (0.5*torch.log(p1) + 0.5*torch.log(p2))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "49c1ce3d-048d-4367-a947-76ab715c655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefnet = torch.load('./human_comparisons/pref_model_500rating_split0.7_acc0.82.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "41806b60-cc3d-47e1-b42e-dfd512fe9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13831/13831 [03:06<00:00, 74.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5836165136288048 5759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing reward model\n",
    "'''\n",
    "acc = []\n",
    "#error_images = []\n",
    "error_count = 0\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    left_idx, right_idx, target = test_set[i]\n",
    "    left_img, right_img = images[left_idx], images[right_idx]\n",
    "    left_pattern, right_pattern = patterns[left_idx], patterns[right_idx]\n",
    "    target = torch.tensor(target).cuda().float()\n",
    "\n",
    "    out1 = prefnet(left_img.cuda().float(), left_pattern.cuda().float())\n",
    "    out2 = prefnet(right_img.cuda().float(), right_pattern.cuda().float())\n",
    "    #print(out1)\n",
    "    #print(out2)\n",
    "    \n",
    "    \n",
    "    if out1 > out2:\n",
    "        y_pred = -1\n",
    "        \n",
    "    else:\n",
    "        y_pred = 1\n",
    "    \n",
    "    #print(y_pred)\n",
    "    #print(\"\")\n",
    "    if y_pred == target:\n",
    "        acc.append(1)\n",
    "    else:\n",
    "        #error_images.append((i, y_pred, target))\n",
    "        error_count += 1\n",
    "        acc.append(0)\n",
    "    #print(out1, out2, target)\n",
    "        \n",
    "print(np.mean(acc), error_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9d0aa508-1489-47c6-9f1c-85d979a0ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(prefnet, './human_comparisons/pref_model_700_random_rating_split0.7_acc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcc0bd-3c16-4279-8032-5f5c4a1b724d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
